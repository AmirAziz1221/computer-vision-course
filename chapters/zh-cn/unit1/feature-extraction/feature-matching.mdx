# 特征匹配

如何将一张图像中检测到的特征与另一张图像中的特征进行匹配？特征匹配旨在通过比较不同图像中的关键属性，来确定它们之间的相似性。特征匹配技术在诸多计算机视觉应用中具有重要价值，其中包括场景理解、图像拼接、目标跟踪以及模式识别等。

## 暴力搜索

假设你面对一大堆拼图碎片，需要从中找到一个能够完美嵌入特定位置的碎片。这与在图像中搜索匹配特征的过程类似。你没有采用任何复杂的策略，而是选择逐一检查每个碎片，直到找到合适的那个。这种直接的方法即为暴力搜索。暴力搜索的优势在于其简单性，无需任何特殊技巧，只需足够的耐心。然而，这种方法可能非常耗时，尤其是在待检查的碎片数量庞大时。在特征匹配的语境下，这种暴力方法相当于将一张图像中的每个像素与另一张图像中的每个像素进行比较，以确定它们是否匹配。虽然这种方法可以确保不遗漏任何可能的匹配，但计算成本可能非常高，特别是对于大型图像而言。

既然我们对暴力匹配有了一个初步的认识，现在让我们深入探讨其算法细节。我们将利用上一章中介绍的描述符，来寻找两张图像中相互匹配的特征。

首先，安装并导入必要的库。

```bash
!pip install opencv-python
```

```python
import cv2
import numpy as np
```

**基于 SIFT 的暴力搜索**

首先，初始化 SIFT 检测器。

```python
sift = cv2.SIFT_create()
```

利用 SIFT 算法检测关键点并计算描述符。

```python
kp1, des1 = sift.detectAndCompute(img1, None)
kp2, des2 = sift.detectAndCompute(img2, None)
```

使用 k 近邻算法寻找匹配项。

```python
bf = cv2.BFMatcher()
matches = bf.knnMatch(des1, des2, k=2)
```

应用比率测试，对最佳匹配进行筛选。

```python
good = []
for m, n in matches:
    if m.distance < 0.75 * n.distance:
        good.append([m])
```

绘制匹配结果。

```python
img3 = cv2.drawMatchesKnn(
    img1, kp1, img2, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/feature-extraction-feature-matching/SIFT.png" alt="SIFT">
</div>

**基于 ORB (二进制) 描述符的暴力搜索**

初始化 ORB 描述符。

```python
orb = cv2.ORB_create()
```

检测关键点并计算描述符。

```python
kp1, des1 = orb.detectAndCompute(img1, None)
kp2, des2 = orb.detectAndCompute(img2, None)
```

由于 ORB 采用二进制描述符，我们使用[汉明距离](https://www.geeksforgeeks.org/hamming-distance-two-strings/)来衡量描述符之间的差异，
汉明距离代表了两个等长字符串之间不同字符的数量。

```python
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
```

现在，寻找匹配项。

```python
matches = bf.match(des1, des2)
```

可以按照距离对匹配项进行排序，如下所示。

```python
matches = sorted(matches, key=lambda x: x.distance)
```

绘制前 n 个最佳匹配。

```python
img3 = cv2.drawMatches(
    img1,
    kp1,
    img2,
    kp2,
    matches[:n],
    None,
    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,
)
```

**快速最近邻逼近搜索库 (FLANN)**

Muja 和 Lowe 在 [Fast Approximate Nearest Neighbors With Automatic Algorithm Configuration](https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_visapp09.pdf) 一文中提出了 FLANN。为了便于理解 FLANN，我们继续沿用之前的拼图游戏示例。想象一下，你面对一个巨大的拼图，数百个碎片散落在各处。你的目标是根据这些碎片之间的匹配程度，将它们组织起来。FLANN 并非随机尝试匹配碎片，而是采用了一些巧妙的技巧，以快速找出哪些碎片最有可能组合在一起。FLANN 通过寻找近似相似的碎片来简化流程，而不是尝试将每个碎片与所有其他碎片进行比较。这意味着它可以对哪些碎片可能很好地组合在一起进行有根据的猜测，即使它们不是完全匹配。在底层，FLANN 使用一种称为 k-D 树的数据结构。可以将其想象成以一种特殊的方式组织拼图碎片。FLANN 不是将每个碎片与所有其他碎片进行比较，而是将它们排列成一种树状结构，从而加快了查找匹配项的速度。在 k-D 树的每个节点中，FLANN 将具有相似特征的碎片放在一起。这就像将具有相似形状或颜色的拼图碎片分类到不同的堆中。这样，当你在寻找匹配项时，你可以快速检查最有可能具有相似碎片的堆。假设你正在寻找一块“天空”碎片。FLANN 不会搜索所有碎片，而是会引导你到 k-D 树中的正确位置，在那里对天空颜色的碎片进行排序。FLANN 还会根据拼图碎片的特征来调整其策略。如果你有一个有很多颜色的拼图，它会专注于颜色特征。或者，如果它是一个具有复杂形状的拼图，它会注意这些形状。通过在查找匹配特征时平衡速度和准确性，FLANN 显着提高了查询效率。

首先，创建一个字典来指定我们将使用的算法。对于 SIFT 或 SURF，其形式如下。

```python
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
```

对于 ORB，将使用论文中建议的参数。

```python
FLANN_INDEX_LSH = 6
index_params = dict(
    algorithm=FLANN_INDEX_LSH, table_number=12, key_size=20, multi_probe_level=2
)
```

我们还需要创建一个字典来指定要访问的最大叶节点数量，如下所示。

```python
search_params = dict(checks=50)
```

初始化 SIFT 检测器。

```python
sift = cv2.SIFT_create()
```

使用 SIFT 算法检测关键点并计算描述符。

```python
kp1, des1 = sift.detectAndCompute(img1, None)
kp2, des2 = sift.detectAndCompute(img2, None)
```

现在，定义 FLANN 的参数。其中，`trees` 参数代表你想要构建的 k-D 树的数量。

```python
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
search_params = dict(checks=50)
flann = cv2.FlannBasedMatcher(index_params, search_params)

matches = flann.knnMatch(des1, des2, k=2)
```

为了只绘制优秀的匹配项，创建一个掩码（mask）。

```python
matchesMask = [[0, 0] for i in range(len(matches))]
```

执行比率测试，以筛选出优秀的匹配项。

```python
for i, (m, n) in enumerate(matches):
    if m.distance < 0.7 * n.distance:
        matchesMask[i] = [1, 0]
```

现在，可视化匹配结果。

```python
draw_params = dict(
    matchColor=(0, 255, 0),
    singlePointColor=(255, 0, 0),
    matchesMask=matchesMask,
    flags=cv2.DrawMatchesFlags_DEFAULT,
)

img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, **draw_params)
```

![FLANN](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/feature-extraction-feature-matching/FLANN.png)

## 基于 Transformer 的局部特征匹配 (LoFTR)

Sun 等人在 [LoFTR：使用 Transformer 的无检测器局部特征匹配](https://arxiv.org/pdf/2104.00680.pdf) 一文中提出了 LoFTR。与传统的特征匹配方法不同，
LoFTR 没有使用特征检测器，而是采用了一种基于学习的方法来进行特征匹配。

为了便于理解，我们继续使用拼图示例。LoFTR 并非简单地逐像素比较图像，而是寻找每张图像中的特定关键点或特征，类似于识别每个拼图碎片的角和边。正如一位拼图高手可能会专注于独特的标记一样，LoFTR 会识别一张图像中的这些独特点，它们可能是关键的地标或突出的结构。正如我们已经了解到的，匹配算法处理旋转或比例变化的能力至关重要。即使某个特征被旋转或调整大小，LoFTR 仍然可以识别它，这就像解决碎片可能被翻转或调整的拼图游戏。当 LoFTR 匹配特征时，它会分配一个相似度分数，以指示特征的对齐程度。较高的分数意味着更好的匹配，类似于给一个拼图碎片与另一个拼图碎片的匹配程度打分。
LoFTR 还可以应对某些变换，这意味着它可以处理光照、角度或透视的变化。这在处理可能在不同条件下拍摄的图像时至关重要。LoFTR 稳健匹配特征的能力使其对于图像拼接等任务非常有用，在这种任务中，你可以通过识别和连接共同特征来无缝地组合多个图像。

我们可以使用 [Kornia](https://github.com/kornia/kornia) 库来寻找两张图像中使用 LoFTR 匹配的特征。

```bash
!pip install kornia kornia-rs kornia_moons opencv-python --upgrade
```

导入必要的库。

```python
import cv2
import kornia as K
import kornia.feature as KF
import matplotlib.pyplot as plt
import numpy as np
import torch
from kornia_moons.viz import draw_LAF_matches
```

加载图像并调整大小。

```python
from kornia.feature import LoFTR

img1 = K.io.load_image(image1.jpg, K.io.ImageLoadType.RGB32)[None, ...]
img2 = K.io.load_image(image2.jpg, K.io.ImageLoadType.RGB32)[None, ...]

img1 = K.geometry.resize(img1, (512, 512), antialias=True)
img2 = K.geometry.resize(img2, (512, 512), antialias=True)
```

指定图像类型，区分“室内”和“室外”场景。

```python
matcher = LoFTR(pretrained="outdoor")
```

LoFTR 仅适用于灰度图像，因此将图像转换为灰度图像。

```python
input_dict = {
    "image0": K.color.rgb_to_grayscale(img1),
    "image1": K.color.rgb_to_grayscale(img2),
}
```

执行推理过程。

```python
with torch.inference_mode():
    correspondences = matcher(input_dict)
```

使用随机抽样一致性算法 ([RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus)) 剔除错误的对应关系，以处理数据中的噪声或异常值。

```python
mkpts0 = correspondences["keypoints0"].cpu().numpy()
mkpts1 = correspondences["keypoints1"].cpu().numpy()
Fm, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)
inliers = inliers > 0
```

最后，可视化匹配结果。

```python
draw_LAF_matches(
    KF.laf_from_center_scale_ori(
        torch.from_numpy(mkpts0).view(1, -1, 2),
        torch.ones(mkpts0.shape[0]).view(1, -1, 1, 1),
        torch.ones(mkpts0.shape[0]).view(1, -1, 1),
    ),
    KF.laf_from_center_scale_ori(
        torch.from_numpy(mkpts1).view(1, -1, 2),
        torch.ones(mkpts1.shape[0]).view(1, -1, 1, 1),
        torch.ones(mkpts1.shape[0]).view(1, -1, 1),
    ),
    torch.arange(mkpts0.shape[0]).view(-1, 1).repeat(1, 2),
    K.tensor_to_image(img1),
    K.tensor_to_image(img2),
    inliers,
    draw_dict={
        "inlier_color": (0.1, 1, 0.1, 0.5),
        "tentative_color": None,
        "feature_color": (0.2, 0.2, 1, 0.5),
        "vertical": False,
    },
)
```

最佳匹配以绿色显示，不太确定的匹配以蓝色显示。

![LoFTR](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/feature-extraction-feature-matching/LoFTR.png)

## 资源和延伸阅读

- [FLANN Github](https://github.com/flann-lib/flann)
- [Image Matching Using SIFT, SURF, BRIEF and ORB: Performance Comparison for Distorted Images](https://arxiv.org/pdf/1710.02726.pdf)
- [ORB (Oriented FAST and Rotated BRIEF) tutorial](https://docs.opencv.org/4.x/d1/d89/tutorial_py_orb.html)
- [Kornia tutorial on Image Matching](https://kornia.github.io/tutorials/nbs/image_matching.html)
- [LoFTR Github](https://github.com/zju3dv/LoFTR)
- [OpenCV Github](https://github.com/opencv/opencv-python)
- [OpenCV Feature Matching Tutorial](https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html)
- [OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching](https://arxiv.org/abs/2204.08870)
