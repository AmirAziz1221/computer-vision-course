# 迁移学习

在深入探讨迁移学习和微调对神经网络的意义之前，我们不妨以乐器为例。特雷门琴是一种音色诡异的电子乐器，常用于营造惊悚恐怖氛围。这种乐器极难掌握，演奏者需要通过双手在两个天线之间的移动来控制音高和音量。由于其极高的学习门槛，一种名为 Tannerin（又称滑管特雷门琴或电子特雷门琴）的乐器应运而生，它能发出类似的声音，但演奏难度却大大降低。演奏者只需移动琴身侧面的滑块至所需频率即可发出相应的音高。然而，演奏 Tannerin 仍然需要一定的学习过程，除非你恰好会演奏长号。演奏长号者对于 Tannerin 的滑块操作可谓是轻车熟路，因为它与长号上的伸缩滑管机制异曲同工。下图从左至右依次为：特雷门琴、Tannerin 和长号。

![特雷门琴、Tannerin 和长号](
https://huggingface.co/datasets/hf-vision/course-assets/resolve/d0096005da7fe2eb3bfbd3d0047e9ac7bd499cf0/transfer_learning.png)

在这个例子中，长号演奏者有效地利用了其演奏长号所习得的知识来驾驭 Tannerin。他将从一种乐器中获得的知识迁移到了另一种乐器之上。这一概念同样适用于神经网络。例如，一个经过猫狗分类训练的神经网络，可以将其所学知识用于识别其他动物。这种方法之所以奏效，是因为网络在模型中学习特征的方式具有一定的通用性。换言之，用于分类狗的已学习特征，同样可以应用于马的分类。我们正是利用模型已有的知识来完成不同的任务。

迁移学习的关键在于先验知识对于新任务的“有用性”。因此，我们所探索的特征需要具备足够的通用性，以适应新的应用场景。回到乐器的例子，如果一位萨克斯管演奏者想要学习 Tannerin，其长号演奏经验的借鉴意义可能并不大。长号演奏者所拥有的关键优势在于他对滑块位置的直观理解。

然而，萨克斯管演奏者并非从零开始。他已经掌握了音乐理论、节奏和节拍等通用知识。这些技能使其比从未接触过任何乐器的人更具优势。演奏乐器的行为本身就能赋予演奏者一套通用的技能，这些技能在各种乐器中都有用武之地。这种跨领域（在我们的例子中是乐器）的泛化能力使得模型的学习速度远快于从头开始训练。

## 迁移学习与微调

让我们明确区分一下本文所讨论的几个概念。长号演奏者几乎不需要任何额外的训练即可演奏 Tannerin。他其实早已掌握了相关技能，只是自己并未意识到。而萨克斯管演奏者则需要通过一定的训练来微调其技能，才能胜任 Tannerin 的演奏。用深度学习的术语来说，长号演奏者使用的是一个开箱即用的模型，这便是所谓的迁移学习。而那些需要更多训练才能掌握的模型，例如我们的萨克斯管演奏者，则需要进行微调。

在微调模型时，我们不必训练所有部分，而可以只训练那些表现欠佳的部分。以一个包含三个部分的计算机视觉模型为例：[特征提取、特征增强和最终任务](https://huggingface.co/docs/transformers/main/main_classes/backbones)。在这种情况下，我们可以直接沿用相同的特征提取和特征增强模块，而无需进行任何重新训练。因此，我们只需专注于重新训练最终任务模块。

如果在微调最终任务后，结果仍不尽如人意，我们仍然无需重新训练整个特征提取部分。一个折衷的方案是只重新训练顶层的权重。在卷积神经网络中，层级越高，其特征就越具有任务和数据集的针对性。换句话说，第一个卷积层中的特征更具通用性，而最后一层则更为具体。用演奏者的例子来类比，这相当于我们无需向一位经验丰富的萨克斯管演奏家讲解音乐理论，而只需教会他如何在 Tannerin 上改变音高。

## 关于迁移学习的注意事项

我们的例子也揭示了一个有趣的现象。由于特雷门琴过于难学，人们发明了一种更容易演奏的乐器，即 Tannerin，它能发出几乎相同的声音。两者的输出结果几乎相同，但 Tannerin 所需的训练时间却大大缩短。在计算机视觉领域，我们可以先进行物体检测，以确定图像中狗的位置，然后再构建一个分类器来识别狗的品种，而不是试图直接构建一个能够完成所有任务的分类器。

最后，迁移学习并非万能的性能增强剂。在我们的例子中，演奏一种乐器可能有助于我们学习另一种乐器，但也可能适得其反。一种乐器的固有模式和不良习惯可能会阻碍另一种乐器的学习进度。如果这些不良习惯根深蒂固，那么一个新手可能会在经过相同训练量后超越这位经验丰富的演奏者。如果你的演奏者们固执己见，或许是时候另觅良才了。

## 迁移学习与自训练

当缺乏足够的标记数据来从头开始重新训练模型时，迁移学习便能大放异彩。沿用之前的例子，我们可以认为，如果有足够的时间，一个只上了几节课的演奏者可以通过自己的不断练习来掌握乐器，而无需老师的监督。在深度学习中，这种部分或完全依靠自身学习的方式被称为自训练。它允许我们同时使用标记数据（课程）和未标记数据（演奏者自己练习）来训练模型，从而完成学习任务。

虽然我们不会在本节中深入探讨自训练的概念，但在此提及它是为了给你提供更多的思路。当迁移学习效果不佳且标记数据稀缺时，[自训练可能会非常有帮助](https://doi.org/10.48550/arXiv.2006.06882)。这些概念也并非互斥，一位经验丰富的演奏者可能只需要几节课就能在一个新的乐器中变得自主，并在没有监督的情况下进行训练。事实证明，我们的深度学习模型也是如此。

## 资源

- 欲了解为何迁移学习更经济、更快速、更环保，请参阅[本课程中关于 NLP 的第一部分。](https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#transfer-learning)

- [查看可用的预训练模型列表。](https://huggingface.co/models)
